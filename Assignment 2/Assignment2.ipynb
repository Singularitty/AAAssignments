{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fcab862",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Models and selection methods\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "# Binary classifier metrics\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, matthews_corrcoef, confusion_matrix, accuracy_score\n",
    "#Pré-Processamento de dados\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, PowerTransformer\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.feature_selection import SelectFromModel, SequentialFeatureSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d0df717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estatisticas para classificadores\n",
    "def printClassResults(truth, preds):\n",
    "    print(\"The Accuracy is: %7.4f\" % accuracy_score(truth, preds))\n",
    "    print(\"The Precision is: %7.4f\" % precision_score(truth, preds))\n",
    "    print(\"The Recall is: %7.4f\" % recall_score(truth, preds))\n",
    "    print(\"The F1 score is: %7.4f\" % f1_score(truth, preds))\n",
    "    matthews = matthews_corrcoef(truth, preds)\n",
    "    print(\"The Matthews correlation coefficient is: %7.4f\" % matthews)\n",
    "    print()\n",
    "    print(\"This is the Confusion Matrix\")\n",
    "    print(pd.DataFrame(confusion_matrix(truth, preds)))\n",
    "\n",
    "# Previsao de resultados com cross validation\n",
    "def CrossValidation(X_TRAIN, y_TRAIN, kf, model):\n",
    "    TRUTH=None\n",
    "    PREDS=None\n",
    "    for train_index, test_index in kf.split(X_TRAIN):\n",
    "        X_train, X_test = X_TRAIN[train_index], X_TRAIN[test_index]\n",
    "        y_train, y_test = y_TRAIN[train_index], y_TRAIN[test_index]\n",
    "        temp_model = clone(model)\n",
    "        temp_model.fit(X_train, y_train)\n",
    "        preds = temp_model.predict(X_test)\n",
    "        if TRUTH is None:\n",
    "            PREDS=preds\n",
    "            TRUTH=y_test\n",
    "        else:\n",
    "            PREDS=np.hstack((PREDS, preds))\n",
    "            TRUTH=np.hstack((TRUTH, y_test))\n",
    "    return (TRUTH, PREDS)\n",
    "    \n",
    "# Model testing rapido\n",
    "def naif_model_testing(X_train_bio, y_train_bio):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_train_bio, y_train_bio, test_size=0.25, random_state=27)\n",
    "    rfr= RandomForestClassifier(n_jobs=8)\n",
    "    rfr.fit(X_train, y_train)\n",
    "    dtr= DecisionTreeClassifier(max_depth=5)\n",
    "    dtr.fit(X_train, y_train)\n",
    "    lmr=LogisticRegression(n_jobs=8)\n",
    "    lmr.fit(X_train, y_train)\n",
    "    rf_preds=rfr.predict(X_test)\n",
    "    dt_preds=dtr.predict(X_test)\n",
    "    lr_preds=lmr.predict(X_test)\n",
    "    scores = [f1_score(y_test, rf_preds),f1_score(y_test, dt_preds),f1_score(y_test, lr_preds)]\n",
    "    print(\"F1 RFs: %7.4f\" % f1_score(y_test, rf_preds))\n",
    "    print(\"F1 DTs: %7.4f\" % f1_score(y_test, dt_preds))\n",
    "    print(\"F1 LRs: %7.4f\" % f1_score(y_test, lr_preds))\n",
    "    print(\"F1 Avg:  %7.4f\" % (sum(scores) / len(scores)))\n",
    "    return (sum(scores) / len(scores))\n",
    "\n",
    "#Escolhas de features metodo stepwise\n",
    "def Step_for(X_train, X_test, y_train):\n",
    "    \n",
    "    N,M=X_train.shape\n",
    "\n",
    "    #Vamos usar random forests\n",
    "    rfr=RandomForestClassifier(random_state=45, n_jobs=8)\n",
    "    sfs = SequentialFeatureSelector(rfr, n_features_to_select=10)\n",
    "    sfs.fit(X_train, y_train)\n",
    "\n",
    "    #get the relevant columns\n",
    "    features=sfs.get_support()\n",
    "    Features_selected =np.arange(M)[features]\n",
    "    print(\"The features selected are columns: \", Features_selected)\n",
    "\n",
    "    nX_train=sfs.transform(X_train)\n",
    "    nX_test=sfs.transform(X_test)\n",
    "\n",
    "    f1_avg = naif_model_testing(nX_train, y_train)\n",
    "    return (f1_avg, nX_train, nX_test)\n",
    "    \n",
    "def ML_Sel(X_train, X_test, y_train, thresh):\n",
    "    N,M=X_train.shape\n",
    "\n",
    "    rfr=RandomForestClassifier(random_state=45, n_jobs=8)\n",
    "    sel = SelectFromModel(estimator=rfr, threshold=thresh)\n",
    "    sel.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"Default threshold: \", sel.threshold_)\n",
    "    features=sel.get_support()\n",
    "    Features_selected =np.arange(M)[features]\n",
    "    print(\"The features selected are columns: \", Features_selected)\n",
    "    nX_train=sel.transform(X_train)\n",
    "    nX_test=sel.transform(X_test)\n",
    "    f1_avg = naif_model_testing(nX_train, y_train)\n",
    "    return (f1_avg, nX_train, nX_test)\n",
    "\n",
    "#Função de peso para o KNN\n",
    "def gaussian(dsts):\n",
    "    kernel_width = .5\n",
    "    weights = np.exp(-(dsts**2)/kernel_width)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491c9542",
   "metadata": {},
   "source": [
    "## Pré-Processamento dos dados\n",
    "Preparação do dataset - importação, normalização e preenchimento dos missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "650c2619",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Criar dataframe\n",
    "bio_a = pd.read_csv('biodegradable_a.csv')\n",
    "#Separação das 41 variáveis do y\n",
    "X_bio_a=bio_a.drop(columns=[\"Biodegradable\"])\n",
    "y_bio_a=bio_a['Biodegradable'].apply(lambda x : 1 if x == 'RB' else 0)\n",
    "#Converter para numpy array\n",
    "Xc_bio= X_bio_a.to_numpy()\n",
    "yc_bio= y_bio_a.to_numpy()\n",
    "# Divisão do dataset em training set e independent validation set\n",
    "X_bio_train, X_bio_test, y_bio_train, y_bio_test = train_test_split(Xc_bio, yc_bio, test_size=0.25, random_state=512)\n",
    "# Kfold\n",
    "kf = KFold(n_splits=16, shuffle=True, random_state = 274)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a81d9f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converter os tipos de floats para outros, para puder determinar as categoricas pelo tipo Int64\n",
    "X_train_temp = pd.DataFrame(X_bio_train).convert_dtypes()\n",
    "X_test_temp = pd.DataFrame(X_bio_test).convert_dtypes()\n",
    "\n",
    "# Imputting das variaveis categoricas utilizando a moda\n",
    "imputer_categoricas = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "\n",
    "# Selecionar as colunas com variaveis Int64\n",
    "imputer_categoricas.fit(X_train_temp.select_dtypes(\"Int64\"))\n",
    "#Criar um dataframe com as varaiveis categoricas sem missing values\n",
    "X_train_categorical = imputer_categoricas.transform(X_train_temp.select_dtypes(\"Int64\"))\n",
    "X_test_categorical = imputer_categoricas.transform(X_test_temp.select_dtypes(\"Int64\"))\n",
    "\n",
    "#Criar um dataframe com as variaveis continuas\n",
    "X_train_continuos = X_train_temp.select_dtypes(exclude=\"Int64\")\n",
    "X_test_continuos = X_test_temp.select_dtypes(exclude=\"Int64\")\n",
    "\n",
    "# Juntar os dataframes das categoricas e continuas com os indices de coluna originais, dar sort das colunas por indice, e converter para float para puder fazer scaling\n",
    "X_bio_train_step1 = X_train_continuos.join(pd.DataFrame(X_train_categorical, columns=X_train_temp.select_dtypes(\"Int64\").columns)).sort_index(axis=1).astype(float)\n",
    "X_bio_test_step1 = X_test_continuos.join(pd.DataFrame(X_test_categorical, columns=X_train_temp.select_dtypes(\"Int64\").columns)).sort_index(axis=1).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657f391e",
   "metadata": {},
   "source": [
    "Passamos agora à normalização dos dados. Vão ser escolhidos os seguintes métodos de normalização para comparar mais tarde: MinMax Scaler, Standard Scaler e Power Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d380d929",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Passemos à normalização dos dados\n",
    "scaler_p = PowerTransformer()\n",
    "scaler_st = StandardScaler()\n",
    "scaler_min= MinMaxScaler()\n",
    "\n",
    "#Transformar dados\n",
    "X_bio_train_p=scaler_p.fit_transform(X_bio_train_step1)\n",
    "X_bio_test_p=scaler_p.fit_transform(X_bio_test_step1)\n",
    "\n",
    "X_bio_train_st=scaler_st.fit_transform(X_bio_train_step1)\n",
    "X_bio_test_st=scaler_st.fit_transform(X_bio_test_step1)\n",
    "\n",
    "X_bio_train_min=scaler_min.fit_transform(X_bio_train_step1)\n",
    "X_bio_test_min=scaler_min.fit_transform(X_bio_test_step1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90d5585",
   "metadata": {},
   "source": [
    "Com os dados normalizados, podemos passar à imputação dos valores em falta. Escolheu-se o método de imputação utilizando o K-Nearest Neighbours em todos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23553b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tratamento dos Missing values -> Utilizar Imputação de KNN\n",
    "\n",
    "imputer_continuas = KNNImputer(n_neighbors=3, weights=\"uniform\")\n",
    "\n",
    "\n",
    "datasets_scaled = []\n",
    "\n",
    "imputer_continuas.fit(X_bio_train_p)\n",
    "X_bio_train_p=imputer_continuas.transform(X_bio_train_p)\n",
    "X_bio_test_p=imputer_continuas.transform(X_bio_test_p)\n",
    "datasets_scaled.append((\"PowerTransformer\",X_bio_train_p, X_bio_test_p))\n",
    "\n",
    "imputer_continuas.fit(X_bio_train_st)\n",
    "imputer_categoricas.fit(X_bio_train_st)\n",
    "X_bio_train_st=imputer_continuas.transform(X_bio_train_st)\n",
    "X_bio_test_st=imputer_continuas.transform(X_bio_test_st)\n",
    "datasets_scaled.append((\"StandardScaler\",X_bio_train_st, X_bio_test_st))\n",
    "\n",
    "imputer_continuas.fit(X_bio_train_min)\n",
    "X_bio_train_min=imputer_continuas.transform(X_bio_train_min)\n",
    "X_bio_test_min=imputer_continuas.transform(X_bio_test_min)\n",
    "datasets_scaled.append((\"MinMax\",X_bio_train_min, X_bio_test_min))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3de335",
   "metadata": {},
   "source": [
    "Resta apenas ver quais são as variáveis mais relevantes. Para tal, vamos utilizar dois métodos diferentes e posteriormente comparar: Stepwise Feature Selection e Random Forests para a seleção de Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "689a3a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling: PowerTransformer\n",
      "Stepwise\n",
      "The features selected are columns:  [ 2  5  6  7 10 15 17 33 35 36]\n",
      "F1 RFs:  0.9822\n",
      "F1 DTs:  0.9704\n",
      "F1 LRs:  0.9517\n",
      "F1 Avg:   0.9681\n",
      "Random Forests\n",
      "Default threshold:  0.035\n",
      "The features selected are columns:  [ 0  2  4  5  6 10 21 26 33 35 40]\n",
      "F1 RFs:  0.9790\n",
      "F1 DTs:  0.9646\n",
      "F1 LRs:  0.9634\n",
      "F1 Avg:   0.9690\n",
      "\n",
      "Scaling: StandardScaler\n",
      "Stepwise\n",
      "The features selected are columns:  [ 2  3  4  5 10 15 22 31 33 37]\n",
      "F1 RFs:  0.9748\n",
      "F1 DTs:  0.9667\n",
      "F1 LRs:  0.9696\n",
      "F1 Avg:   0.9704\n",
      "Random Forests\n",
      "Default threshold:  0.035\n",
      "The features selected are columns:  [ 0  2  4  5  6 10 21 33 35 40]\n",
      "F1 RFs:  0.9789\n",
      "F1 DTs:  0.9652\n",
      "F1 LRs:  0.9690\n",
      "F1 Avg:   0.9710\n",
      "\n",
      "Scaling: MinMax\n",
      "Stepwise\n",
      "The features selected are columns:  [ 2  4  5  6 10 15 22 33 37 40]\n",
      "F1 RFs:  0.9782\n",
      "F1 DTs:  0.9709\n",
      "F1 LRs:  0.9521\n",
      "F1 Avg:   0.9671\n",
      "Random Forests\n",
      "Default threshold:  0.035\n",
      "The features selected are columns:  [ 0  2  4  5  6 10 21 33 35 40]\n",
      "F1 RFs:  0.9782\n",
      "F1 DTs:  0.9639\n",
      "F1 LRs:  0.9558\n",
      "F1 Avg:   0.9660\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datasets_reduced = []\n",
    "for name, x_train_scaled, x_test_scaled in datasets_scaled:\n",
    "    print(\"Scaling:\", name)\n",
    "    print(\"Stepwise\")\n",
    "    datasets_reduced.append(Step_for(x_train_scaled, x_test_scaled, y_bio_train))\n",
    "    print(\"Random Forests\")\n",
    "    datasets_reduced.append(ML_Sel(x_train_scaled, x_test_scaled, y_bio_train, 0.035))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00e2a080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# escolher o conjunto de treino com maior f1_avg\n",
    "_,X_train, X_test = max(datasets_reduced, key= lambda x : x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6e26f4",
   "metadata": {},
   "source": [
    "## Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263094c5",
   "metadata": {},
   "source": [
    "Nesta segunda parte iremos criar modelos que consigam prevêr se um químico é ou não biodegradável. Iremos também otimizar estes modelos consoante os seus hiperparâmetros. Os modelos a ser utilizados são: Decision Tree, Regressão Logística, KNN, SVM, Random Forests e XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4920bfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos guardar aqui os modelos e seu respetivo mathews correlaction coef\n",
    "models = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee80dc8f",
   "metadata": {},
   "source": [
    "**Decision Tree Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c19ca46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores Parâmetros: {'criterion': 'gini', 'max_depth': 12, 'min_samples_leaf': 1, 'min_samples_split': 5} \n",
      "\n",
      "The Accuracy is:  0.9299\n",
      "The Precision is:  0.9608\n",
      "The Recall is:  0.9569\n",
      "The F1 score is:  0.9588\n",
      "The Matthews correlation coefficient is:  0.7222\n",
      "\n",
      "This is the Confusion Matrix\n",
      "     0    1\n",
      "0  129   38\n",
      "1   42  932\n"
     ]
    }
   ],
   "source": [
    "params = [\n",
    "    {\"max_depth\" : [6,8,10,12,14,16,18,20,22,24,26,28,30],\n",
    "    \"min_samples_leaf\" : [1,2,5,10, 15, 20],\n",
    "    \"min_samples_split\" : [2,5,10, 20, 25, 30],\n",
    "    \"criterion\":['gini','entropy']}]\n",
    "\n",
    "grid_search_treeclass = GridSearchCV(\n",
    "    DecisionTreeClassifier(), params, scoring=\"f1\", cv=kf, n_jobs=-1)\n",
    "\n",
    "grid_search_treeclass.fit(X_train,  y_bio_train)\n",
    "print(\"Melhores Parâmetros:\", grid_search_treeclass.best_params_,\"\\n\")\n",
    "(Truth, Preds) = CrossValidation(X_test, y_bio_test, kf, grid_search_treeclass.best_estimator_)\n",
    "printClassResults(Truth, Preds)\n",
    "models.append((matthews_corrcoef(Truth, Preds), grid_search_treeclass.best_estimator_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708fde32",
   "metadata": {},
   "source": [
    "**Regressão Logistica**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80b7de8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores Parâmetros: {'C': 0.6000000000000001, 'max_iter': 999999} \n",
      "\n",
      "The Accuracy is:  0.9457\n",
      "The Precision is:  0.9479\n",
      "The Recall is:  0.9908\n",
      "The F1 score is:  0.9689\n",
      "The Matthews correlation coefficient is:  0.7675\n",
      "\n",
      "This is the Confusion Matrix\n",
      "     0    1\n",
      "0  114   53\n",
      "1    9  965\n"
     ]
    }
   ],
   "source": [
    "params = [\n",
    "    {\"C\" : [x*0.1 for x in range(1,11)],\n",
    "    \"max_iter\" : [999999]}]\n",
    "\n",
    "grid_search_log = GridSearchCV(\n",
    "    LogisticRegression(), params, scoring=\"f1\", cv=kf, n_jobs=8)\n",
    "\n",
    "grid_search_log.fit(X_train, y_bio_train)\n",
    "print(\"Melhores Parâmetros:\", grid_search_log.best_params_,\"\\n\")\n",
    "(Truth, Preds) = CrossValidation(X_test, y_bio_test, kf, grid_search_log.best_estimator_)\n",
    "printClassResults(Truth, Preds)\n",
    "models.append((matthews_corrcoef(Truth, Preds), grid_search_log.best_estimator_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8bf7dd",
   "metadata": {},
   "source": [
    "**Gaussian Naive Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb5e48b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy is:  0.9229\n",
      "The Precision is:  0.9567\n",
      "The Recall is:  0.9528\n",
      "The F1 score is:  0.9547\n",
      "The Matthews correlation coefficient is:  0.6945\n",
      "\n",
      "This is the Confusion Matrix\n",
      "     0    1\n",
      "0  125   42\n",
      "1   46  928\n"
     ]
    }
   ],
   "source": [
    "gaussNB = GaussianNB()\n",
    "gaussNB.fit(X_train, y_bio_train)\n",
    "(Truth, Preds) = CrossValidation(X_test, y_bio_test, kf, gaussNB)\n",
    "printClassResults(Truth, Preds)\n",
    "models.append((matthews_corrcoef(Truth, Preds), gaussNB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52e69eb",
   "metadata": {},
   "source": [
    "**KNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7330422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores Parâmetros: {'n_neighbors': 4, 'weights': <function gaussian at 0x00000207401E2670>} \n",
      "\n",
      "The Accuracy is:  0.9483\n",
      "The Precision is:  0.9561\n",
      "The Recall is:  0.9846\n",
      "The F1 score is:  0.9702\n",
      "The Matthews correlation coefficient is:  0.7817\n",
      "\n",
      "This is the Confusion Matrix\n",
      "     0    1\n",
      "0  123   44\n",
      "1   15  959\n"
     ]
    }
   ],
   "source": [
    "params = [{\"n_neighbors\": [1,2,3,4,5,6,7,8,9,10],\n",
    "          \"weights\":[\"uniform\", \"distance\", gaussian]}]\n",
    "\n",
    "grid_search_knn = GridSearchCV(\n",
    "    KNeighborsClassifier(), params, scoring=\"f1\", cv=kf, n_jobs=8)\n",
    "\n",
    "grid_search_knn.fit(X_train, y_bio_train)\n",
    "print(\"Melhores Parâmetros:\", grid_search_knn.best_params_,\"\\n\")\n",
    "(Truth, Preds) = CrossValidation(X_test, y_bio_test, kf, grid_search_knn.best_estimator_)\n",
    "printClassResults(Truth, Preds)\n",
    "models.append((matthews_corrcoef(Truth, Preds), grid_search_knn.best_estimator_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973e5504",
   "metadata": {},
   "source": [
    "**SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1266e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores Parâmetros: {'C': 10, 'gamma': 0.5, 'kernel': 'rbf'} \n",
      "\n",
      "The Accuracy is:  0.9483\n",
      "The Precision is:  0.9626\n",
      "The Recall is:  0.9774\n",
      "The F1 score is:  0.9699\n",
      "The Matthews correlation coefficient is:  0.7862\n",
      "\n",
      "This is the Confusion Matrix\n",
      "     0    1\n",
      "0  130   37\n",
      "1   22  952\n"
     ]
    }
   ],
   "source": [
    "params =[{\"kernel\": ['linear','rbf','sigmoid'],\n",
    "         \"gamma\": [0.1,0.5,1,10,100,1000],\n",
    "         \"C\": [0.1,1,10,100,1000]}]\n",
    "\n",
    "grid_search_svc = GridSearchCV(\n",
    "    SVC(), params, scoring=\"f1\", cv=kf, n_jobs=8)\n",
    "\n",
    "grid_search_svc.fit(X_train, y_bio_train)\n",
    "print(\"Melhores Parâmetros:\", grid_search_svc.best_params_,\"\\n\")\n",
    "(Truth, Preds) = CrossValidation(X_test, y_bio_test, kf, grid_search_svc.best_estimator_)\n",
    "printClassResults(Truth, Preds)\n",
    "models.append((matthews_corrcoef(Truth, Preds), grid_search_svc.best_estimator_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b86767",
   "metadata": {},
   "source": [
    "**RANDOM FOREST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d097c788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores Parâmetros: {'criterion': 'entropy', 'max_depth': 22, 'min_samples_leaf': 5, 'min_samples_split': 10, 'n_estimators': 1000} \n",
      "\n",
      "The Accuracy is:  0.9457\n",
      "The Precision is:  0.9488\n",
      "The Recall is:  0.9897\n",
      "The F1 score is:  0.9688\n",
      "The Matthews correlation coefficient is:  0.7677\n",
      "\n",
      "This is the Confusion Matrix\n",
      "     0    1\n",
      "0  115   52\n",
      "1   10  964\n"
     ]
    }
   ],
   "source": [
    "params = [\n",
    "    {\"n_estimators\": [10,100,1000],\n",
    "    \"max_depth\" : [4,10,16,22,28],\n",
    "    \"min_samples_leaf\" : [5,10,20],\n",
    "    \"min_samples_split\" : [5,10,20,30],\n",
    "    \"criterion\":['gini','entropy']}]\n",
    "\n",
    "grid_search_rfc = GridSearchCV(\n",
    "    RandomForestClassifier(), params, scoring=\"f1\", cv=kf, n_jobs=8)\n",
    "\n",
    "grid_search_rfc.fit(X_train,  y_bio_train)\n",
    "print(\"Melhores Parâmetros:\", grid_search_rfc.best_params_,\"\\n\")\n",
    "(Truth, Preds) = CrossValidation(X_test, y_bio_test, kf, grid_search_rfc.best_estimator_)\n",
    "printClassResults(Truth, Preds)\n",
    "models.append((matthews_corrcoef(Truth, Preds), grid_search_rfc.best_estimator_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8e2f12",
   "metadata": {},
   "source": [
    "**ADABOOST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "644e03a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores Parâmetros: {'base_estimator': RandomForestClassifier(max_depth=5), 'learning_rate': 1, 'n_estimators': 100} \n",
      "\n",
      "The Accuracy is:  0.9562\n",
      "The Precision is:  0.9602\n",
      "The Recall is:  0.9897\n",
      "The F1 score is:  0.9747\n",
      "The Matthews correlation coefficient is:  0.8158\n",
      "\n",
      "This is the Confusion Matrix\n",
      "     0    1\n",
      "0  127   40\n",
      "1   10  964\n"
     ]
    }
   ],
   "source": [
    "params =[{\"n_estimators\": [10,100],\n",
    "          \"learning_rate\": [0.01,0.1,1],\n",
    "          \"base_estimator\": [GaussianNB(), RandomForestClassifier(max_depth=5)]}]\n",
    "\n",
    "grid_search_ada = GridSearchCV(\n",
    "    AdaBoostClassifier(), params, scoring=\"f1\", cv=kf, n_jobs=-1)\n",
    "\n",
    "grid_search_ada.fit(X_train, y_bio_train)\n",
    "print(\"Melhores Parâmetros:\", grid_search_ada.best_params_,\"\\n\")\n",
    "(Truth, Preds) = CrossValidation(X_test, y_bio_test, kf, grid_search_ada.best_estimator_)\n",
    "printClassResults(Truth, Preds)\n",
    "models.append((matthews_corrcoef(Truth, Preds), grid_search_ada.best_estimator_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4acb4e",
   "metadata": {},
   "source": [
    "**XGBOOST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69f6e856",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:47:24] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:47:27] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:47:28] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:47:30] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:47:31] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:47:32] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:47:33] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:47:35] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:47:36] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:47:37] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:47:39] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:47:40] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:47:41] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:47:43] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:47:44] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:47:45] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:47:47] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "params=[{\"n_estimators\": [10,100,500,1000],\n",
    "        \"max_depth\" : [4,8,12,16,20,24,28],\n",
    "        \"learning_rate\":[0.01,0.1,0.5,1]}]\n",
    "\n",
    "grid_search_xgb = GridSearchCV(\n",
    "    XGBClassifier(), params, scoring=\"f1\", cv=kf, n_jobs=8)\n",
    "\n",
    "grid_search_xgb.fit(X_train, y_bio_train)\n",
    "(Truth, Preds) = CrossValidation(X_test, y_bio_test, kf, grid_search_xgb.best_estimator_)\n",
    "models.append((matthews_corrcoef(Truth, Preds), grid_search_xgb.best_estimator_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2dcfbaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores Parâmetros: {'learning_rate': 0.01, 'max_depth': 8, 'n_estimators': 1000} \n",
      "\n",
      "The Accuracy is:  0.9492\n",
      "The Precision is:  0.9589\n",
      "The Recall is:  0.9825\n",
      "The F1 score is:  0.9706\n",
      "The Matthews correlation coefficient is:  0.7869\n",
      "\n",
      "This is the Confusion Matrix\n",
      "     0    1\n",
      "0  126   41\n",
      "1   17  957\n"
     ]
    }
   ],
   "source": [
    "print(\"Melhores Parâmetros:\", grid_search_xgb.best_params_,\"\\n\")\n",
    "printClassResults(Truth, Preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf8b7703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(base_estimator=RandomForestClassifier(max_depth=5),\n",
       "                   learning_rate=1, n_estimators=100)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Escolhemos automaticamente o melhor modelo baseado no matthews coeficient\n",
    "# Lembremos que models = [(Matthews_coef, model_1),...]\n",
    "best_model = max(models, key= lambda x: x[0])[1]\n",
    "best_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
