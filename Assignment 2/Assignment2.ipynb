{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fcab862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Models and selection methods\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "# Binary classifier metrics\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, matthews_corrcoef, confusion_matrix, accuracy_score\n",
    "# Linear regression metrics\n",
    "from sklearn.metrics import explained_variance_score, mean_squared_error, max_error, mean_absolute_error\n",
    "from scipy.stats import pearsonr\n",
    "#Pré-Processamento de dados\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, PowerTransformer\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.feature_selection import SelectFromModel, SequentialFeatureSelector\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d0df717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estatisticas para classificadores\n",
    "def printClassResults(truth, preds):\n",
    "    print(\"The Accuracy is: %7.4f\" % accuracy_score(truth, preds))\n",
    "    print(\"The Precision is: %7.4f\" % precision_score(truth, preds))\n",
    "    print(\"The Recall is: %7.4f\" % recall_score(truth, preds))\n",
    "    print(\"The F1 score is: %7.4f\" % f1_score(truth, preds))\n",
    "    matthews = matthews_corrcoef(truth, preds)\n",
    "    print(\"The Matthews correlation coefficient is: %7.4f\" % matthews)\n",
    "    print()\n",
    "    print(\"This is the Confusion Matrix\")\n",
    "    print(pd.DataFrame(confusion_matrix(truth, preds)))\n",
    "\n",
    "# Previsao de resultados com cross validation\n",
    "def CrossValidation(X_TRAIN, y_TRAIN, kf, model):\n",
    "    TRUTH=None\n",
    "    PREDS=None\n",
    "    for train_index, test_index in kf.split(X_TRAIN):\n",
    "        X_train, X_test = X_TRAIN[train_index], X_TRAIN[test_index]\n",
    "        y_train, y_test = y_TRAIN[train_index], y_TRAIN[test_index]\n",
    "        temp_model = clone(model)\n",
    "        temp_model.fit(X_train, y_train)\n",
    "        preds = temp_model.predict(X_test)\n",
    "        if TRUTH is None:\n",
    "            PREDS=preds\n",
    "            TRUTH=y_test\n",
    "        else:\n",
    "            PREDS=np.hstack((PREDS, preds))\n",
    "            TRUTH=np.hstack((TRUTH, y_test))\n",
    "    return (TRUTH, PREDS)\n",
    "    \n",
    "# Model testing rapido\n",
    "def naif_model_testing(X_train, X_test, y_train, y_test):\n",
    "    rfr= RandomForestClassifier(n_jobs=-1)\n",
    "    rfr.fit(X_train, y_train)\n",
    "    dtr= DecisionTreeClassifier(max_depth=5)\n",
    "    dtr.fit(X_train, y_train)\n",
    "    lmr=LogisticRegression(n_jobs=-1)\n",
    "    lmr.fit(X_train, y_train)\n",
    "    rf_preds=rfr.predict(X_test)\n",
    "    dt_preds=dtr.predict(X_test)\n",
    "    lr_preds=lmr.predict(X_test)\n",
    "    scores = [f1_score(y_test, rf_preds),f1_score(y_test, dt_preds),f1_score(y_test, lr_preds)]\n",
    "    print(\"F1 RFs: %7.4f\" % f1_score(y_test, rf_preds))\n",
    "    print(\"F1 DTs: %7.4f\" % f1_score(y_test, dt_preds))\n",
    "    print(\"F1 LRs: %7.4f\" % f1_score(y_test, lr_preds))\n",
    "    print(\"F1 Avg:  %7.4f\" % (sum(scores) / len(scores)))\n",
    "    return (sum(scores) / len(scores))\n",
    "\n",
    "#Escolhas de features metodo \n",
    "def Step_for(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    N,M=X_train.shape\n",
    "\n",
    "    #Vamos usar random forests\n",
    "    rfr=RandomForestClassifier(random_state=45)\n",
    "    sfs = SequentialFeatureSelector(rfr, n_features_to_select=10)\n",
    "    sfs.fit(X_train, y_train)\n",
    "\n",
    "    #get the relevant columns\n",
    "    features=sfs.get_support()\n",
    "    Features_selected =np.arange(M)[features]\n",
    "    print(\"The features selected are columns: \", Features_selected)\n",
    "\n",
    "    nX_train=sfs.transform(X_train)\n",
    "    nX_test=sfs.transform(X_test)\n",
    "\n",
    "    f1_avg = naif_model_testing(nX_train, nX_test, y_train, y_test)\n",
    "    return (f1_avg, nX_train, nX_test)\n",
    "    \n",
    "def ML_Sel(X_train, X_test, y_train, y_test, thresh):\n",
    "    N,M=X_train.shape\n",
    "\n",
    "    rfr=RandomForestClassifier(random_state=45, n_jobs=-1)\n",
    "    sel = SelectFromModel(estimator=rfr, threshold=thresh)\n",
    "    sel.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"Default threshold: \", sel.threshold_)\n",
    "    features=sel.get_support()\n",
    "    Features_selected =np.arange(M)[features]\n",
    "    print(\"The features selected are columns: \", Features_selected)\n",
    "    nX_train=sel.transform(X_train)\n",
    "    nX_test=sel.transform(X_test)\n",
    "    f1_avg = naif_model_testing(nX_train, nX_test, y_train, y_test)\n",
    "    return (f1_avg, nX_train, nX_test)\n",
    "\n",
    "#Função de peso para o KNN\n",
    "def gaussian(dsts):\n",
    "    kernel_width = .5\n",
    "    weights = np.exp(-(dsts**2)/kernel_width)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491c9542",
   "metadata": {},
   "source": [
    "## Pré-Processamento dos dados\n",
    "Preparação do dataset - importação, normalização e preenchimento dos missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "650c2619",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Criar dataframe\n",
    "bio_a = pd.read_csv('biodegradable_a.csv')\n",
    "#Separação das 41 variáveis do y\n",
    "X_bio_a=bio_a.drop(columns=[\"Biodegradable\"])\n",
    "y_bio_a=bio_a['Biodegradable'].apply(lambda x : 1 if x == 'RB' else 0)\n",
    "#Converter para numpy array\n",
    "Xc_bio= X_bio_a.to_numpy()\n",
    "yc_bio= y_bio_a.to_numpy()\n",
    "# Divisão do dataset em training set e independent validation set\n",
    "X_bio_train, X_bio_test, y_bio_train, y_bio_test = train_test_split(Xc_bio, yc_bio, test_size=0.25, random_state=512)\n",
    "# Kfold\n",
    "kf = KFold(n_splits=16, shuffle=True, random_state = 274)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a81d9f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converter os tipos de floats para outros, para puder determinar as categoricas pelo tipo Int64\n",
    "X_train_temp = pd.DataFrame(X_bio_train).convert_dtypes()\n",
    "X_test_temp = pd.DataFrame(X_bio_test).convert_dtypes()\n",
    "\n",
    "# Imputting das variaveis categoricas utilizando a moda\n",
    "imputer_categoricas = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "\n",
    "# Selecionar as colunas com variaveis Int64\n",
    "imputer_categoricas.fit(X_train_temp.select_dtypes(\"Int64\"))\n",
    "#Criar um dataframe com as varaiveis categoricas sem missing values\n",
    "X_train_categorical = imputer_categoricas.transform(X_train_temp.select_dtypes(\"Int64\"))\n",
    "X_test_categorical = imputer_categoricas.transform(X_test_temp.select_dtypes(\"Int64\"))\n",
    "\n",
    "#Criar um dataframe com as variaveis continuas\n",
    "X_train_continuos = X_train_temp.select_dtypes(exclude=\"Int64\")\n",
    "X_test_continuos = X_test_temp.select_dtypes(exclude=\"Int64\")\n",
    "\n",
    "# Juntar os dataframes das categoricas e continuas com os indices de coluna originais, dar sort das colunas por indice, e converter para float para puder fazer scaling\n",
    "X_bio_train_step1 = X_train_continuos.join(pd.DataFrame(X_train_categorical, columns=X_train_temp.select_dtypes(\"Int64\").columns)).sort_index(axis=1).astype(float)\n",
    "X_bio_test_step1 = X_test_continuos.join(pd.DataFrame(X_test_categorical, columns=X_train_temp.select_dtypes(\"Int64\").columns)).sort_index(axis=1).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657f391e",
   "metadata": {},
   "source": [
    "Passamos agora à normalização dos dados. Vão ser escolhidos os seguintes métodos de normalização para comparar mais tarde: MinMax Scaler, Standard Scaler e Power Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d380d929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.857579</td>\n",
       "      <td>0.168452</td>\n",
       "      <td>-0.298474</td>\n",
       "      <td>-0.087486</td>\n",
       "      <td>-0.329914</td>\n",
       "      <td>-0.193061</td>\n",
       "      <td>-0.766057</td>\n",
       "      <td>-0.176119</td>\n",
       "      <td>-1.048452</td>\n",
       "      <td>1.235903</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.110105</td>\n",
       "      <td>-0.457263</td>\n",
       "      <td>-0.427509</td>\n",
       "      <td>1.321538</td>\n",
       "      <td>-0.636159</td>\n",
       "      <td>2.246219</td>\n",
       "      <td>-0.468445</td>\n",
       "      <td>-0.949614</td>\n",
       "      <td>-0.222886</td>\n",
       "      <td>-0.214078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.854459</td>\n",
       "      <td>-1.019003</td>\n",
       "      <td>-0.298474</td>\n",
       "      <td>-0.087486</td>\n",
       "      <td>-0.329914</td>\n",
       "      <td>-0.193061</td>\n",
       "      <td>-0.766057</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.001919</td>\n",
       "      <td>-0.615911</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.110105</td>\n",
       "      <td>-0.457263</td>\n",
       "      <td>-0.427509</td>\n",
       "      <td>1.553357</td>\n",
       "      <td>-2.135502</td>\n",
       "      <td>0.987608</td>\n",
       "      <td>2.123623</td>\n",
       "      <td>-2.097699</td>\n",
       "      <td>-0.222886</td>\n",
       "      <td>-0.214078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.430395</td>\n",
       "      <td>1.193057</td>\n",
       "      <td>-0.298474</td>\n",
       "      <td>-0.087486</td>\n",
       "      <td>-0.329914</td>\n",
       "      <td>-0.193061</td>\n",
       "      <td>-0.766057</td>\n",
       "      <td>0.291224</td>\n",
       "      <td>-1.048452</td>\n",
       "      <td>0.109793</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.110105</td>\n",
       "      <td>-0.457263</td>\n",
       "      <td>-0.427509</td>\n",
       "      <td>-0.874182</td>\n",
       "      <td>0.281114</td>\n",
       "      <td>0.540742</td>\n",
       "      <td>-0.468445</td>\n",
       "      <td>0.064084</td>\n",
       "      <td>-0.222886</td>\n",
       "      <td>-0.214078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.421222</td>\n",
       "      <td>-0.036376</td>\n",
       "      <td>-0.298474</td>\n",
       "      <td>-0.087486</td>\n",
       "      <td>-0.329914</td>\n",
       "      <td>-0.193061</td>\n",
       "      <td>1.293260</td>\n",
       "      <td>1.092849</td>\n",
       "      <td>-1.048452</td>\n",
       "      <td>0.109793</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.110105</td>\n",
       "      <td>-0.457263</td>\n",
       "      <td>-0.427509</td>\n",
       "      <td>-0.874182</td>\n",
       "      <td>0.456898</td>\n",
       "      <td>0.227531</td>\n",
       "      <td>-0.468445</td>\n",
       "      <td>0.252883</td>\n",
       "      <td>-0.222886</td>\n",
       "      <td>-0.214078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.751657</td>\n",
       "      <td>-1.775834</td>\n",
       "      <td>-0.298474</td>\n",
       "      <td>-0.087486</td>\n",
       "      <td>-0.329914</td>\n",
       "      <td>-0.193061</td>\n",
       "      <td>-0.766057</td>\n",
       "      <td>-0.466588</td>\n",
       "      <td>0.350802</td>\n",
       "      <td>-1.585803</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.110105</td>\n",
       "      <td>-0.457263</td>\n",
       "      <td>-0.427509</td>\n",
       "      <td>0.821645</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.140070</td>\n",
       "      <td>-0.468445</td>\n",
       "      <td>-1.096429</td>\n",
       "      <td>-0.222886</td>\n",
       "      <td>-0.214078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3418</th>\n",
       "      <td>1.664765</td>\n",
       "      <td>0.591478</td>\n",
       "      <td>3.349907</td>\n",
       "      <td>-0.087486</td>\n",
       "      <td>-0.329914</td>\n",
       "      <td>-0.193061</td>\n",
       "      <td>1.293260</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.048452</td>\n",
       "      <td>0.711807</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.110105</td>\n",
       "      <td>-0.457263</td>\n",
       "      <td>-0.427509</td>\n",
       "      <td>1.321538</td>\n",
       "      <td>2.082672</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.468445</td>\n",
       "      <td>1.506523</td>\n",
       "      <td>-0.222886</td>\n",
       "      <td>-0.214078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3419</th>\n",
       "      <td>0.244807</td>\n",
       "      <td>-0.641362</td>\n",
       "      <td>-0.298474</td>\n",
       "      <td>-0.087486</td>\n",
       "      <td>-0.329914</td>\n",
       "      <td>-0.193061</td>\n",
       "      <td>-0.766057</td>\n",
       "      <td>-0.292831</td>\n",
       "      <td>1.001919</td>\n",
       "      <td>0.109793</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.110105</td>\n",
       "      <td>-0.457263</td>\n",
       "      <td>-0.427509</td>\n",
       "      <td>1.553357</td>\n",
       "      <td>-0.191679</td>\n",
       "      <td>-0.552582</td>\n",
       "      <td>-0.468445</td>\n",
       "      <td>0.648379</td>\n",
       "      <td>-0.222886</td>\n",
       "      <td>-0.214078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3420</th>\n",
       "      <td>-0.562774</td>\n",
       "      <td>0.804821</td>\n",
       "      <td>-0.298474</td>\n",
       "      <td>-0.087486</td>\n",
       "      <td>-0.329914</td>\n",
       "      <td>-0.193061</td>\n",
       "      <td>-0.766057</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.350802</td>\n",
       "      <td>0.109793</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.110105</td>\n",
       "      <td>-0.457263</td>\n",
       "      <td>-0.427509</td>\n",
       "      <td>1.321538</td>\n",
       "      <td>0.404293</td>\n",
       "      <td>2.169303</td>\n",
       "      <td>-0.468445</td>\n",
       "      <td>-0.276885</td>\n",
       "      <td>-0.222886</td>\n",
       "      <td>-0.214078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3421</th>\n",
       "      <td>0.755840</td>\n",
       "      <td>-0.060657</td>\n",
       "      <td>-0.298474</td>\n",
       "      <td>-0.087486</td>\n",
       "      <td>-0.329914</td>\n",
       "      <td>-0.193061</td>\n",
       "      <td>1.293260</td>\n",
       "      <td>0.813510</td>\n",
       "      <td>1.402128</td>\n",
       "      <td>-1.585803</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.110105</td>\n",
       "      <td>-0.457263</td>\n",
       "      <td>-0.427509</td>\n",
       "      <td>-0.874182</td>\n",
       "      <td>0.397339</td>\n",
       "      <td>-1.456878</td>\n",
       "      <td>-0.468445</td>\n",
       "      <td>0.039927</td>\n",
       "      <td>-0.222886</td>\n",
       "      <td>-0.214078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3422</th>\n",
       "      <td>1.054821</td>\n",
       "      <td>-0.193085</td>\n",
       "      <td>-0.298474</td>\n",
       "      <td>-0.087486</td>\n",
       "      <td>-0.329914</td>\n",
       "      <td>-0.193061</td>\n",
       "      <td>1.293260</td>\n",
       "      <td>0.414988</td>\n",
       "      <td>1.001919</td>\n",
       "      <td>1.235903</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.110105</td>\n",
       "      <td>-0.457263</td>\n",
       "      <td>-0.427509</td>\n",
       "      <td>-0.874182</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.167892</td>\n",
       "      <td>-0.468445</td>\n",
       "      <td>0.764431</td>\n",
       "      <td>4.486606</td>\n",
       "      <td>-0.214078</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3423 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6   \\\n",
       "0    -1.857579  0.168452 -0.298474 -0.087486 -0.329914 -0.193061 -0.766057   \n",
       "1    -1.854459 -1.019003 -0.298474 -0.087486 -0.329914 -0.193061 -0.766057   \n",
       "2     0.430395  1.193057 -0.298474 -0.087486 -0.329914 -0.193061 -0.766057   \n",
       "3     0.421222 -0.036376 -0.298474 -0.087486 -0.329914 -0.193061  1.293260   \n",
       "4    -0.751657 -1.775834 -0.298474 -0.087486 -0.329914 -0.193061 -0.766057   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "3418  1.664765  0.591478  3.349907 -0.087486 -0.329914 -0.193061  1.293260   \n",
       "3419  0.244807 -0.641362 -0.298474 -0.087486 -0.329914 -0.193061 -0.766057   \n",
       "3420 -0.562774  0.804821 -0.298474 -0.087486 -0.329914 -0.193061 -0.766057   \n",
       "3421  0.755840 -0.060657 -0.298474 -0.087486 -0.329914 -0.193061  1.293260   \n",
       "3422  1.054821 -0.193085 -0.298474 -0.087486 -0.329914 -0.193061  1.293260   \n",
       "\n",
       "            7         8         9   ...        31        32        33  \\\n",
       "0    -0.176119 -1.048452  1.235903  ... -0.110105 -0.457263 -0.427509   \n",
       "1          NaN  1.001919 -0.615911  ... -0.110105 -0.457263 -0.427509   \n",
       "2     0.291224 -1.048452  0.109793  ... -0.110105 -0.457263 -0.427509   \n",
       "3     1.092849 -1.048452  0.109793  ... -0.110105 -0.457263 -0.427509   \n",
       "4    -0.466588  0.350802 -1.585803  ... -0.110105 -0.457263 -0.427509   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "3418       NaN -1.048452  0.711807  ... -0.110105 -0.457263 -0.427509   \n",
       "3419 -0.292831  1.001919  0.109793  ... -0.110105 -0.457263 -0.427509   \n",
       "3420       NaN  0.350802  0.109793  ... -0.110105 -0.457263 -0.427509   \n",
       "3421  0.813510  1.402128 -1.585803  ... -0.110105 -0.457263 -0.427509   \n",
       "3422  0.414988  1.001919  1.235903  ... -0.110105 -0.457263 -0.427509   \n",
       "\n",
       "            34        35        36        37        38        39        40  \n",
       "0     1.321538 -0.636159  2.246219 -0.468445 -0.949614 -0.222886 -0.214078  \n",
       "1     1.553357 -2.135502  0.987608  2.123623 -2.097699 -0.222886 -0.214078  \n",
       "2    -0.874182  0.281114  0.540742 -0.468445  0.064084 -0.222886 -0.214078  \n",
       "3    -0.874182  0.456898  0.227531 -0.468445  0.252883 -0.222886 -0.214078  \n",
       "4     0.821645       NaN -1.140070 -0.468445 -1.096429 -0.222886 -0.214078  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "3418  1.321538  2.082672       NaN -0.468445  1.506523 -0.222886 -0.214078  \n",
       "3419  1.553357 -0.191679 -0.552582 -0.468445  0.648379 -0.222886 -0.214078  \n",
       "3420  1.321538  0.404293  2.169303 -0.468445 -0.276885 -0.222886 -0.214078  \n",
       "3421 -0.874182  0.397339 -1.456878 -0.468445  0.039927 -0.222886 -0.214078  \n",
       "3422 -0.874182       NaN -0.167892 -0.468445  0.764431  4.486606 -0.214078  \n",
       "\n",
       "[3423 rows x 41 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Passemos à normalização dos dados\n",
    "scaler_p = PowerTransformer()\n",
    "scaler_st = StandardScaler()\n",
    "scaler_min= MinMaxScaler()\n",
    "\n",
    "#Transformar dados\n",
    "X_bio_train_p=scaler_p.fit_transform(X_bio_train_step1)\n",
    "X_bio_test_p=scaler_p.fit_transform(X_bio_test_step1)\n",
    "\n",
    "X_bio_train_st=scaler_st.fit_transform(X_bio_train_step1)\n",
    "X_bio_test_st=scaler_st.fit_transform(X_bio_test_step1)\n",
    "\n",
    "X_bio_train_min=scaler_min.fit_transform(X_bio_train_step1)\n",
    "X_bio_test_min=scaler_min.fit_transform(X_bio_test_step1)\n",
    "\n",
    "pd.DataFrame(X_bio_train_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90d5585",
   "metadata": {},
   "source": [
    "Com os dados normalizados, podemos passar à imputação dos valores em falta. Escolheu-se o método de imputação utilizando o K-Nearest Neighbours em todos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23553b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tratamento dos Missing values -> Utilizar Imputação de KNN\n",
    "\n",
    "imputer_continuas = KNNImputer(n_neighbors=3, weights=\"uniform\")\n",
    "\n",
    "\n",
    "datasets_scaled = []\n",
    "\n",
    "#imputer.fit(X_bio_train_p)\n",
    "#X_bio_train_p=imputer.transform(X_bio_train_p)\n",
    "#X_bio_test_p=imputer.transform(X_bio_test_p)\n",
    "#datasets_scaled.append((\"PowerTransformer\",X_bio_train_p, X_bio_test_p))\n",
    "\n",
    "imputer_continuas.fit(X_bio_train_st)\n",
    "imputer_categoricas.fit(X_bio_train_st)\n",
    "X_bio_train_st=imputer_continuas.transform(X_bio_train_st)\n",
    "X_bio_test_st=imputer_continuas.transform(X_bio_test_st)\n",
    "datasets_scaled.append((\"StandardScaler\",X_bio_train_st, X_bio_test_st))\n",
    "\n",
    "#imputer.fit(X_bio_train_min)\n",
    "#X_bio_train_min=imputer.transform(X_bio_train_min)\n",
    "#X_bio_test_min=imputer.transform(X_bio_test_min)\n",
    "#datasets_scaled.append((\"MinMax\",X_bio_train_min, X_bio_test_min))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3de335",
   "metadata": {},
   "source": [
    "Resta apenas ver quais são as variáveis mais relevantes. Para tal, vamos utilizar dois métodos diferentes e posteriormente comparar: Stepwise Feature Selection e Random Forests para a seleção de Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "689a3a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling: StandardScaler\n",
      "Random Forests\n",
      "Default threshold:  0.035\n",
      "The features selected are columns:  [ 0  2  4  5  6 10 21 33 35 40]\n",
      "F1 RFs:  0.9765\n",
      "F1 DTs:  0.9722\n",
      "F1 LRs:  0.9662\n",
      "F1 Avg:   0.9716\n"
     ]
    }
   ],
   "source": [
    "datasets_reduced = []\n",
    "for name, x_train_scaled, x_test_scaled in datasets_scaled:\n",
    "    print(\"Scaling:\", name)\n",
    "    #print(\"Stepwise\")\n",
    "    #datasets_reduced.append((name,Step_for(x_train_scaled, x_test_scaled, y_bio_train, y_bio_test)))\n",
    "    print(\"Random Forests\")\n",
    "    datasets_reduced.append(ML_Sel(x_train_scaled, x_test_scaled, y_bio_train, y_bio_test, 0.035))\n",
    "\n",
    "#O melhor deu o ML_Sel(StandardScaler): [ 0  2  4  5  6 10 21 33 35 40]\n",
    "#F1 RFs:  0.9760\n",
    "#F1 DTs:  0.9722\n",
    "#F1 LRs:  0.9651\n",
    "#F1 Avg:   0.9711"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00e2a080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# escolher o conjunto de treino com maior f1_avg\n",
    "_,X_train, X_test = max(datasets_reduced, key= lambda x : x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6e26f4",
   "metadata": {},
   "source": [
    "## Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263094c5",
   "metadata": {},
   "source": [
    "Nesta segunda parte iremos criar modelos que consigam prevêr se um químico é ou não biodegradável. Iremos também otimizar estes modelos consoante os seus hiperparâmetros. Os modelos a ser utilizados são: Decision Tree, Regressão Logística, KNN, SVM, Random Forests e XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4920bfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos guardar aqui os modelos e seu respetivo mathews correlaction coef\n",
    "models = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee80dc8f",
   "metadata": {},
   "source": [
    "**Decision Tree Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c19ca46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores Parâmetros: {'criterion': 'gini', 'max_depth': 14, 'min_samples_leaf': 5, 'min_samples_split': 25} \n",
      "\n",
      "The Accuracy is:  0.9480\n",
      "The Precision is:  0.9658\n",
      "The Recall is:  0.9719\n",
      "The F1 score is:  0.9689\n",
      "The Matthews correlation coefficient is:  0.8110\n",
      "\n",
      "This is the Confusion Matrix\n",
      "     0     1\n",
      "0  474    98\n",
      "1   80  2771\n"
     ]
    }
   ],
   "source": [
    "params = [\n",
    "    {\"max_depth\" : [6,8,10,12,14,16,18,20,22,24,26,28,30],\n",
    "    \"min_samples_leaf\" : [1,2,5,10, 15, 20],\n",
    "    \"min_samples_split\" : [2,5,10, 20, 25, 30],\n",
    "    \"criterion\":['gini','entropy']}]\n",
    "\n",
    "grid_search_treeclass = GridSearchCV(\n",
    "    DecisionTreeClassifier(), params, scoring=\"f1\", cv=kf, n_jobs=-1)\n",
    "\n",
    "grid_search_treeclass.fit(X_train,  y_bio_train)\n",
    "print(\"Melhores Parâmetros:\", grid_search_treeclass.best_params_,\"\\n\")\n",
    "(Truth, Preds) = CrossValidation(X_train, y_bio_train, kf, grid_search_treeclass.best_estimator_)\n",
    "printClassResults(Truth, Preds)\n",
    "models.append((matthews_corrcoef(Truth, Preds), grid_search_treeclass.best_estimator_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708fde32",
   "metadata": {},
   "source": [
    "**Regressão Logistica**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80b7de8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores Parâmetros: {'C': 0.6000000000000001, 'max_iter': 999999} \n",
      "\n",
      "The Accuracy is:  0.9366\n",
      "The Precision is:  0.9419\n",
      "The Recall is:  0.9846\n",
      "The F1 score is:  0.9628\n",
      "The Matthews correlation coefficient is:  0.7581\n",
      "\n",
      "This is the Confusion Matrix\n",
      "     0     1\n",
      "0  399   173\n",
      "1   44  2807\n"
     ]
    }
   ],
   "source": [
    "params = [\n",
    "    {\"C\" : [x*0.1 for x in range(1,11)],\n",
    "    \"max_iter\" : [999999]}]\n",
    "\n",
    "grid_search_log = GridSearchCV(\n",
    "    LogisticRegression(), params, scoring=\"f1\", cv=kf, n_jobs=-1)\n",
    "\n",
    "grid_search_log.fit(X_train, y_bio_train)\n",
    "print(\"Melhores Parâmetros:\", grid_search_log.best_params_,\"\\n\")\n",
    "(Truth, Preds) = CrossValidation(X_train, y_bio_train, kf, grid_search_log.best_estimator_)\n",
    "printClassResults(Truth, Preds)\n",
    "models.append((matthews_corrcoef(Truth, Preds), grid_search_log.best_estimator_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8bf7dd",
   "metadata": {},
   "source": [
    "**Gaussian Naive Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb5e48b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy is:  0.9179\n",
      "The Precision is:  0.9547\n",
      "The Recall is:  0.9463\n",
      "The F1 score is:  0.9505\n",
      "The Matthews correlation coefficient is:  0.7104\n",
      "\n",
      "This is the Confusion Matrix\n",
      "     0     1\n",
      "0  444   128\n",
      "1  153  2698\n"
     ]
    }
   ],
   "source": [
    "gaussNB = GaussianNB()\n",
    "gaussNB.fit(X_train, y_bio_train)\n",
    "(Truth, Preds) = CrossValidation(X_train, y_bio_train, kf, gaussNB)\n",
    "printClassResults(Truth, Preds)\n",
    "models.append((matthews_corrcoef(Truth, Preds), gaussNB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52e69eb",
   "metadata": {},
   "source": [
    "**KNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7330422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores Parâmetros: {'n_neighbors': 4, 'weights': <function gaussian at 0x000001601F1F5A60>} \n",
      "\n",
      "The Accuracy is:  0.9568\n",
      "The Precision is:  0.9668\n",
      "The Recall is:  0.9818\n",
      "The F1 score is:  0.9742\n",
      "The Matthews correlation coefficient is:  0.8407\n",
      "\n",
      "This is the Confusion Matrix\n",
      "     0     1\n",
      "0  476    96\n",
      "1   52  2799\n"
     ]
    }
   ],
   "source": [
    "params = [{\"n_neighbors\": [1,2,3,4,5,6,7,8,9,10],\n",
    "          \"weights\":[\"uniform\", \"distance\", gaussian]}] #não se usou gaussian pois havia alguns erros\n",
    "\n",
    "grid_search_knn = GridSearchCV(\n",
    "    KNeighborsClassifier(), params, scoring=\"f1\", cv=kf, n_jobs=-1)\n",
    "\n",
    "grid_search_knn.fit(X_train, y_bio_train)\n",
    "print(\"Melhores Parâmetros:\", grid_search_knn.best_params_,\"\\n\")\n",
    "(Truth, Preds) = CrossValidation(X_train, y_bio_train, kf, grid_search_knn.best_estimator_)\n",
    "printClassResults(Truth, Preds)\n",
    "models.append((matthews_corrcoef(Truth, Preds), grid_search_knn.best_estimator_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973e5504",
   "metadata": {},
   "source": [
    "**SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1266e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores Parâmetros: {'C': 10, 'gamma': 0.5, 'kernel': 'rbf'} \n",
      "\n",
      "The Accuracy is:  0.9562\n",
      "The Precision is:  0.9681\n",
      "The Recall is:  0.9797\n",
      "The F1 score is:  0.9738\n",
      "The Matthews correlation coefficient is:  0.8393\n",
      "\n",
      "This is the Confusion Matrix\n",
      "     0     1\n",
      "0  480    92\n",
      "1   58  2793\n"
     ]
    }
   ],
   "source": [
    "params =[{\"kernel\": ['linear','rbf','sigmoid'],\n",
    "         \"gamma\": [0.1,0.5,1,10,100,1000],\n",
    "         \"C\": [0.1,1,10,100,1000]}]\n",
    "\n",
    "grid_search_svc = GridSearchCV(\n",
    "    SVC(), params, scoring=\"f1\", cv=kf, n_jobs=-1)\n",
    "\n",
    "grid_search_svc.fit(X_train, y_bio_train)\n",
    "print(\"Melhores Parâmetros:\", grid_search_svc.best_params_,\"\\n\")\n",
    "(Truth, Preds) = CrossValidation(X_train, y_bio_train, kf, grid_search_svc.best_estimator_)\n",
    "printClassResults(Truth, Preds)\n",
    "models.append((matthews_corrcoef(Truth, Preds), grid_search_svc.best_estimator_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b86767",
   "metadata": {},
   "source": [
    "**RANDOM FOREST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d097c788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores Parâmetros: {'criterion': 'entropy', 'max_depth': 16, 'min_samples_leaf': 5, 'min_samples_split': 10, 'n_estimators': 1000} \n",
      "\n",
      "The Accuracy is:  0.9553\n",
      "The Precision is:  0.9617\n",
      "The Recall is:  0.9856\n",
      "The F1 score is:  0.9735\n",
      "The Matthews correlation coefficient is:  0.8336\n",
      "\n",
      "This is the Confusion Matrix\n",
      "     0     1\n",
      "0  460   112\n",
      "1   41  2810\n"
     ]
    }
   ],
   "source": [
    "params = [\n",
    "    {\"n_estimators\": [10,100,1000],\n",
    "    \"max_depth\" : [4,10,16,22,28],\n",
    "    \"min_samples_leaf\" : [5,10,20],\n",
    "    \"min_samples_split\" : [5,10,20,30],\n",
    "    \"criterion\":['gini','entropy']}]\n",
    "\n",
    "grid_search_rfc = GridSearchCV(\n",
    "    RandomForestClassifier(), params, scoring=\"f1\", cv=kf, n_jobs=-1)\n",
    "\n",
    "grid_search_rfc.fit(X_train,  y_bio_train)\n",
    "print(\"Melhores Parâmetros:\", grid_search_rfc.best_params_,\"\\n\")\n",
    "(Truth, Preds) = CrossValidation(X_train, y_bio_train, kf, grid_search_rfc.best_estimator_)\n",
    "printClassResults(Truth, Preds)\n",
    "models.append((matthews_corrcoef(Truth, Preds), grid_search_rfc.best_estimator_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8e2f12",
   "metadata": {},
   "source": [
    "**ADABOOST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "644e03a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores Parâmetros: {'base_estimator': GaussianNB(), 'learning_rate': 0.1, 'n_estimators': 100} \n",
      "\n",
      "The Accuracy is:  0.9422\n",
      "The Precision is:  0.9480\n",
      "The Recall is:  0.9846\n",
      "The F1 score is:  0.9659\n",
      "The Matthews correlation coefficient is:  0.7810\n",
      "\n",
      "This is the Confusion Matrix\n",
      "     0     1\n",
      "0  418   154\n",
      "1   44  2807\n"
     ]
    }
   ],
   "source": [
    "params =[{\"n_estimators\": [10,100,1000],\n",
    "          \"learning_rate\": [0.01,0.1,0.5,1],\n",
    "          \"base_estimator\": [GaussianNB(), DecisionTreeClassifier()]}]\n",
    "\n",
    "grid_search_ada = GridSearchCV(\n",
    "    AdaBoostClassifier(), params, scoring=\"f1\", cv=kf, n_jobs=-1)\n",
    "\n",
    "grid_search_ada.fit(X_train, y_bio_train)\n",
    "print(\"Melhores Parâmetros:\", grid_search_ada.best_params_,\"\\n\")\n",
    "(Truth, Preds) = CrossValidation(X_train, y_bio_train, kf, grid_search_ada.best_estimator_)\n",
    "printClassResults(Truth, Preds)\n",
    "models.append((matthews_corrcoef(Truth, Preds), grid_search_ada.best_estimator_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4acb4e",
   "metadata": {},
   "source": [
    "**XGBOOST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69f6e856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores Parâmetros: {'learning_rate': 0.01, 'max_depth': 8, 'n_estimators': 1000} \n",
      "\n",
      "The Accuracy is:  0.9609\n",
      "The Precision is:  0.9689\n",
      "The Recall is:  0.9846\n",
      "The F1 score is:  0.9767\n",
      "The Matthews correlation coefficient is:  0.8558\n",
      "\n",
      "This is the Confusion Matrix\n",
      "     0     1\n",
      "0  482    90\n",
      "1   44  2807\n"
     ]
    }
   ],
   "source": [
    "params=[{\"n_estimators\": [10,100,500,1000],\n",
    "        \"max_depth\" : [4,8,12,16,20,24,28],\n",
    "        \"learning_rate\":[0.01,0.1,0.5,1]}]\n",
    "\n",
    "grid_search_xgb = GridSearchCV(\n",
    "    XGBClassifier(), params, scoring=\"f1\", cv=kf, n_jobs=-1)\n",
    "\n",
    "grid_search_xgb.fit(X_train, y_bio_train)\n",
    "print(\"Melhores Parâmetros:\", grid_search_xgb.best_params_,\"\\n\")\n",
    "(Truth, Preds) = CrossValidation(X_train, y_bio_train, kf, grid_search_xgb.best_estimator_)\n",
    "printClassResults(Truth, Preds)\n",
    "models.append((matthews_corrcoef(Truth, Preds), grid_search_xgb.best_estimator_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e944cba",
   "metadata": {},
   "source": [
    "## Validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf8b7703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, feature_types=None, gamma=0, gpu_id=-1,\n",
       "              grow_policy='depthwise', importance_type=None,\n",
       "              interaction_constraints='', learning_rate=0.01, max_bin=256,\n",
       "              max_cat_threshold=64, max_cat_to_onehot=4, max_delta_step=0,\n",
       "              max_depth=8, max_leaves=0, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=1000, n_jobs=0,\n",
       "              num_parallel_tree=1, predictor='auto', random_state=0, ...)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Escolhemos automaticamente o melhor modelo baseado no matthews coeficient\n",
    "best_model = max(models, key= lambda x: x[0])[1]\n",
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "16836edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy is:  0.9614\n",
      "The Precision is:  0.9745\n",
      "The Recall is:  0.9805\n",
      "The F1 score is:  0.9775\n",
      "The Matthews correlation coefficient is:  0.8435\n",
      "\n",
      "This is the Confusion Matrix\n",
      "     0    1\n",
      "0  142   25\n",
      "1   19  955\n"
     ]
    }
   ],
   "source": [
    "IVS_Preds = best_model.predict(X_test)\n",
    "printClassResults(y_bio_test, IVS_Preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf984b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "ca0a6ed6c3687464969dd6147ab8a98b55d02ef7a9aec76841b10aa7e05a39c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
